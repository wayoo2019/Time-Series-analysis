{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "475-Project3.1_Q2.ipynb",
      "provenance": [],
      "mount_file_id": "1hKH7RUmgrLlENldVfAVm1NGpx-GnIWAx",
      "authorship_tag": "ABX9TyPNJDKhLzQwTyn9aswbiVfr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wayoo2019/Time-Series-analysis/blob/main/475_Project3_1_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdmJdUEq2MSN"
      },
      "source": [
        "#Q2.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P14mWFgFvr6-"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FWXnTIiCUHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c6a9b3-0145-41e3-e30e-8a16e01a4fdd"
      },
      "source": [
        "def findFiles(path): \n",
        "    return glob.glob(path)\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "\n",
        "names = {}\n",
        "languages = []\n",
        "\n",
        "\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
        "for filename in findFiles('/content/drive/MyDrive/475 Time Series/Project 3/data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    languages.append(category)\n",
        "    lines = readLines(filename)\n",
        "    names[category] = lines\n",
        "\n",
        "\n",
        "n_categories = len(languages)\n",
        "\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "\n",
        "def nameToTensor(name):\n",
        "    tensor = torch.zeros(len(name), 1, n_letters)\n",
        "    for li, letter in enumerate(name):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
        "        super(RNN, self).__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size = INPUT_SIZE,\n",
        "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
        "            num_layers = N_LAYERS, # number of layers\n",
        "            batch_first = True)\n",
        "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
        "        out = self.out(r_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "n_hidden = 128\n",
        "\n",
        "allnames = [] # Create list of all names and corresponding output language\n",
        "for language in list(names.keys()):\n",
        "    for name in names[language]:\n",
        "        allnames.append([name, language])\n",
        "        \n",
        "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
        "\n",
        "# maxlen = ..... # Add code here to compute the maximum length of string \n",
        "maxlen = 0\n",
        "for name in allnames:\n",
        "  if maxlen <= len(nameToTensor(name[0])):\n",
        "    maxlen = len(nameToTensor(name[0]))\n",
        "padded_length = maxlen\n",
        "\n",
        "                \n",
        "n_letters = len(all_letters)\n",
        "n_categories = len(languages)\n",
        "\n",
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i.item()\n",
        "    return languages[category_i], category_i\n",
        "\n",
        "learning_rate = 0.005\n",
        "rnn = RNN(n_letters, 128, 1, n_categories)\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()  \n",
        "\n",
        "for epoch in range(5):  \n",
        "    batch_size = len(allnames)\n",
        "    random.shuffle(allnames)\n",
        "    \n",
        "    # if \"b_in\" and \"b_out\" are the variable names for input and output tensors, you need to create those\n",
        "    \n",
        "# (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
        "# (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)   \n",
        "    b_in = torch.zeros(batch_size, padded_length, n_letters)\n",
        "    b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)\n",
        "\n",
        "    # (TO DO:) Populate \"b_in\" tensor \n",
        "    # (TO DO:) Populate \"b_out\" tensor\n",
        "    for i, name in enumerate(allnames):\n",
        "      for li, letter in enumerate(name[0]):\n",
        "        b_in[i][li][letterToIndex(letter)] = 1\n",
        "    \n",
        "    for i, name in enumerate(allnames): \n",
        "      b_out[i][languages.index(name[1])] = 1\n",
        "    \n",
        "       \n",
        "    output = rnn(b_in)                               # rnn output\n",
        "    #(TO DO:)\n",
        "    loss = loss_func(output, torch.max(b_out, 1)[1])   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
        "    optimizer.zero_grad()                           # clear gradients for this training step\n",
        "    loss.backward()                                 # backpropagation, compute gradients\n",
        "    optimizer.step()                                # apply gradients\n",
        "        \n",
        "    # Print accuracy\n",
        "    test_output = rnn(b_in)                   # \n",
        "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
        "    test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
        "    accuracy = sum(pred_y == test_y)/batch_size\n",
        "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.2f' % accuracy)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | train loss: 2.8475 | accuracy: 0.47\n",
            "Epoch:  1 | train loss: 2.6160 | accuracy: 0.47\n",
            "Epoch:  2 | train loss: 2.0862 | accuracy: 0.47\n",
            "Epoch:  3 | train loss: 1.9306 | accuracy: 0.47\n",
            "Epoch:  4 | train loss: 1.9574 | accuracy: 0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DBltGNqva-V"
      },
      "source": [
        "#Q2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1aggnLYwaCN"
      },
      "source": [
        "**Batch size = 1000:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrH-D7RLviZ1",
        "outputId": "b1b03518-4667-4392-bf10-624ef5a2e577"
      },
      "source": [
        "BATCH_SIZE = 1000 # assign batch size in the beginning\n",
        "maxlen = 0\n",
        "for name in allnames[:BATCH_SIZE]:\n",
        "  if maxlen <= len(nameToTensor(name[0])):\n",
        "    maxlen = len(nameToTensor(name[0]))\n",
        "padded_length = maxlen\n",
        "\n",
        "for epoch in range(5):  \n",
        "    batch_size = BATCH_SIZE\n",
        "    random.shuffle(allnames)\n",
        "\n",
        "    b_in = torch.zeros(batch_size, padded_length, n_letters)\n",
        "    b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)\n",
        "\n",
        "    for i, name in enumerate(allnames):\n",
        "      for li, letter in enumerate(name[0]):\n",
        "        if i < BATCH_SIZE:\n",
        "          b_in[i][li][letterToIndex(letter)] = 1\n",
        "    \n",
        "    for i, name in enumerate(allnames): \n",
        "      if i < BATCH_SIZE:\n",
        "        b_out[i][languages.index(name[1])] = 1\n",
        "    \n",
        "       \n",
        "    output = rnn(b_in)                               # rnn output\n",
        "    #(TO DO:)\n",
        "    loss = loss_func(output, torch.max(b_out, 1)[1])   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
        "    optimizer.zero_grad()                           # clear gradients for this training step\n",
        "    loss.backward()                                 # backpropagation, compute gradients\n",
        "    optimizer.step()                                # apply gradients\n",
        "        \n",
        "    # Print accuracy\n",
        "    test_output = rnn(b_in)                   # \n",
        "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
        "    test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
        "    accuracy = sum(pred_y == test_y)/batch_size\n",
        "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.2f' % accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | train loss: 1.7807 | accuracy: 0.49\n",
            "Epoch:  1 | train loss: 1.8961 | accuracy: 0.46\n",
            "Epoch:  2 | train loss: 1.8071 | accuracy: 0.48\n",
            "Epoch:  3 | train loss: 1.8563 | accuracy: 0.47\n",
            "Epoch:  4 | train loss: 1.8922 | accuracy: 0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLTTHqGyNE5"
      },
      "source": [
        "**Bacth size = 2000:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imnRkGz-yQl6",
        "outputId": "49bf8f9e-e587-47b1-e475-965c2aead9ce"
      },
      "source": [
        "BATCH_SIZE = 2000 # assign batch size in the beginning\n",
        "maxlen = 0\n",
        "for name in allnames[:BATCH_SIZE]:\n",
        "  if maxlen <= len(nameToTensor(name[0])):\n",
        "    maxlen = len(nameToTensor(name[0]))\n",
        "padded_length = maxlen\n",
        "\n",
        "for epoch in range(5):  \n",
        "    batch_size = BATCH_SIZE\n",
        "    random.shuffle(allnames)\n",
        "\n",
        "    b_in = torch.zeros(batch_size, padded_length, n_letters)\n",
        "    b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)\n",
        "\n",
        "    for i, name in enumerate(allnames):\n",
        "      for li, letter in enumerate(name[0]):\n",
        "        if i < BATCH_SIZE:\n",
        "          b_in[i][li][letterToIndex(letter)] = 1\n",
        "    \n",
        "    for i, name in enumerate(allnames): \n",
        "      if i < BATCH_SIZE:\n",
        "        b_out[i][languages.index(name[1])] = 1\n",
        "    \n",
        "       \n",
        "    output = rnn(b_in)                               # rnn output\n",
        "    #(TO DO:)\n",
        "    loss = loss_func(output, torch.max(b_out, 1)[1])   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
        "    optimizer.zero_grad()                           # clear gradients for this training step\n",
        "    loss.backward()                                 # backpropagation, compute gradients\n",
        "    optimizer.step()                                # apply gradients\n",
        "        \n",
        "    # Print accuracy\n",
        "    test_output = rnn(b_in)                   # \n",
        "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
        "    test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
        "    accuracy = sum(pred_y == test_y)/batch_size\n",
        "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.2f' % accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | train loss: 1.8425 | accuracy: 0.46\n",
            "Epoch:  1 | train loss: 1.8707 | accuracy: 0.45\n",
            "Epoch:  2 | train loss: 1.8620 | accuracy: 0.47\n",
            "Epoch:  3 | train loss: 1.8476 | accuracy: 0.46\n",
            "Epoch:  4 | train loss: 1.8302 | accuracy: 0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq0Kjw0j37n9"
      },
      "source": [
        "**Bacth size = 3000:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miDKSmrI3_7X",
        "outputId": "bb2519a7-9684-4a5b-d6c6-57b4dc2781ab"
      },
      "source": [
        "BATCH_SIZE = 3000 # assign batch size in the beginning\n",
        "maxlen = 0\n",
        "for name in allnames[:BATCH_SIZE]:\n",
        "  if maxlen <= len(nameToTensor(name[0])):\n",
        "    maxlen = len(nameToTensor(name[0]))\n",
        "padded_length = maxlen\n",
        "\n",
        "for epoch in range(5):  \n",
        "    batch_size = BATCH_SIZE\n",
        "    random.shuffle(allnames)\n",
        "\n",
        "    b_in = torch.zeros(batch_size, padded_length, n_letters)\n",
        "    b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)\n",
        "\n",
        "    for i, name in enumerate(allnames):\n",
        "      for li, letter in enumerate(name[0]):\n",
        "        if i < BATCH_SIZE:\n",
        "          b_in[i][li][letterToIndex(letter)] = 1\n",
        "    \n",
        "    for i, name in enumerate(allnames): \n",
        "      if i < BATCH_SIZE:\n",
        "        b_out[i][languages.index(name[1])] = 1\n",
        "    \n",
        "       \n",
        "    output = rnn(b_in)                               # rnn output\n",
        "    #(TO DO:)\n",
        "    loss = loss_func(output, torch.max(b_out, 1)[1])   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
        "    optimizer.zero_grad()                           # clear gradients for this training step\n",
        "    loss.backward()                                 # backpropagation, compute gradients\n",
        "    optimizer.step()                                # apply gradients\n",
        "        \n",
        "    # Print accuracy\n",
        "    test_output = rnn(b_in)                   # \n",
        "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
        "    test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
        "    accuracy = sum(pred_y == test_y)/batch_size\n",
        "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.2f' % accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | train loss: 1.8782 | accuracy: 0.46\n",
            "Epoch:  1 | train loss: 1.8222 | accuracy: 0.48\n",
            "Epoch:  2 | train loss: 1.8957 | accuracy: 0.46\n",
            "Epoch:  3 | train loss: 1.8637 | accuracy: 0.47\n",
            "Epoch:  4 | train loss: 1.8502 | accuracy: 0.47\n"
          ]
        }
      ]
    }
  ]
}